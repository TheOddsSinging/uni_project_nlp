{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "#### Student Name: Zhen Qian\n",
    "#### Student ID: s3888611\n",
    "\n",
    "Date: 03/10/2021\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "* nltk\n",
    "* itertools\n",
    "\n",
    "\n",
    "## Introduction\n",
    "Basic text processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from itertools import chain\n",
    "from nltk.probability import *\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data\n",
    "- xamine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root directory has 37 files/folders\n",
      "they are: ['fast_text_weighted_projected.npy', 'count_vectors.npy', 'fast_text_weighted.npy', 'FT_desc.model', 'fast_text_unweighted.npy', 'task1.ipynb', 'glove_weighted.npy', 'task3.py', 'google_news_unweighted_projected.npy', 'ft_title_weighted.npy', 'glove_weighted_projected.npy', 'count_vectors_title.npy', 'google_title_weighted.npy', 'fast_text_unweighted_projected.npy', 'ft_title_unweighted.npy', 'test.py', 'glove_unweighted_projected.npy', 'test2.py', 'bigram.txt', 'google_title_unweighted.npy', 'classification.ipynb', 'glove', 'job_ads.txt', 'vocab_title.txt', 'google_news_weighted.npy', 'task2_3.ipynb', 'vocab.txt', 'job_categories.npy', 'glove_unweighted.npy', 'google_news_unweighted.npy', 'FT_desc.model.wv.vectors_ngrams.npy', 'count_vectors.txt', 'stopwords_en.txt', 'glove_title_weighted.npy', 'google_news_weighted_projected.npy', 'data', 'glove_title_unweighted.npy']\n"
     ]
    }
   ],
   "source": [
    "# Code to inspect the provided data file...\n",
    "## examine the files and folders in root directory\n",
    "rtdir = [f for f in os.listdir() if not f.startswith('.')]\n",
    "print('root directory has', len(rtdir), 'files/folders')\n",
    "print('they are:', rtdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data folder has 8 files/folders\n",
      "they are: ['Sales', 'Accounting_Finance', 'IT', 'Healthcare_Nursing', 'Teaching', 'PR_Advertising_Marketing', 'Hospitality_Catering', 'Engineering']\n"
     ]
    }
   ],
   "source": [
    "## examine files and folders in the data folder\n",
    "datadir = [f for f in os.listdir('./data') if not f.startswith('.')]\n",
    "print('data folder has', len(datadir), 'files/folders')\n",
    "print('they are:', datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales - 5349 files\n",
      "Accounting_Finance - 7407 files\n",
      "IT - 14353 files\n",
      "Healthcare_Nursing - 8808 files\n",
      "Teaching - 3779 files\n",
      "PR_Advertising_Marketing - 2755 files\n",
      "Hospitality_Catering - 4788 files\n",
      "Engineering - 8210 files\n"
     ]
    }
   ],
   "source": [
    "## examine files in each ad folder\n",
    "### number of files in each folder\n",
    "ads_filenames = {}\n",
    "for folder in datadir:\n",
    "    ads_filenames[folder] = sorted([f for f in os.listdir('./data/'+folder) if f.endswith('.txt')])\n",
    "for key, value in ads_filenames.items():\n",
    "    print(key,'-', len(value), 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define lists to strore ids, categories, webindex, title, and description\n",
    "ads_ids = []\n",
    "ads_categories = []\n",
    "ads_webinds = []\n",
    "ads_titles = []\n",
    "ads_descs = []\n",
    "\n",
    "for folder, filenames in ads_filenames.items():\n",
    "    for filename in filenames:\n",
    "        \n",
    "        ads_ids.append(re.findall('\\d{5}', filename)[0])\n",
    "        ads_categories.append(folder)\n",
    "        \n",
    "        with open('./data/'+folder+'/'+filename, 'r') as f:\n",
    "            ads_text = f.read()\n",
    "        \n",
    "        ads_text = ads_text.splitlines()\n",
    "        ads_webinds = ads_webinds+[line.strip().lstrip('Webindex:').strip() for line in ads_text if line.startswith('Webindex')]\n",
    "        ads_titles = ads_titles+[line.strip().lstrip('Title:').strip() for line in ads_text if line.startswith('Title')]\n",
    "        ads_descs = ads_descs+[line.strip().lstrip('Description:').strip() for line in ads_text if line.startswith('Description')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55449\n",
      "55449\n",
      "55449\n",
      "55449\n",
      "55449\n"
     ]
    }
   ],
   "source": [
    "# check if any files are missed\n",
    "print(len(ads_ids))\n",
    "print(len(ads_categories))\n",
    "print(len(ads_webinds))\n",
    "print(len(ads_titles))\n",
    "print(len(ads_descs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "Perform the required text pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...... Sections and code blocks on basic text pre-processing\n",
    "\n",
    "\n",
    "<span style=\"color: red\"> You might have complex notebook structure in this section, please feel free to create your own notebook structure. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to check the processed results\n",
    "def tokens_stats (list_name):\n",
    "    ads_num = len(list_name)\n",
    "    words_num = len(list(chain.from_iterable(list_name)))\n",
    "    vocabs_num = len(set(chain.from_iterable(list_name)))\n",
    "    ads_length = [len(ad) for ad in list_name]\n",
    "    length_max = max(ads_length)\n",
    "    length_min = min(ads_length)\n",
    "    \n",
    "    print('number of ads:', ads_num)\n",
    "    print('number of words:', words_num)\n",
    "    print('number of vocabularies:', vocabs_num)\n",
    "    print('maximum length of an ad:', length_max)\n",
    "    print('minimum length of an ad:', length_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "ads_descs_tokens = [tokenizer.tokenize(desc) for desc in ads_descs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ads: 55449\n",
      "number of words: 13799127\n",
      "number of vocabularies: 112580\n",
      "maximum length of an ad: 2001\n",
      "minimum length of an ad: 10\n"
     ]
    }
   ],
   "source": [
    "tokens_stats(ads_descs_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert words to lower case\n",
    "ads_descs_tokens = [[token.lower() for token in ad] for ad in ads_descs_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ads: 55449\n",
      "number of words: 13799127\n",
      "number of vocabularies: 89591\n",
      "maximum length of an ad: 2001\n",
      "minimum length of an ad: 10\n"
     ]
    }
   ],
   "source": [
    "tokens_stats(ads_descs_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words with length less than 2\n",
    "ads_descs_tokens = [[token for token in ad if len(token)>1] for ad in ads_descs_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ads: 55449\n",
      "number of words: 13342925\n",
      "number of vocabularies: 89565\n",
      "maximum length of an ad: 1919\n",
      "minimum length of an ad: 10\n"
     ]
    }
   ],
   "source": [
    "tokens_stats(ads_descs_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "with open('stopwords_en.txt', 'r') as f:\n",
    "    stopwords = f.read()\n",
    "\n",
    "stopwords = stopwords.splitlines()\n",
    "ads_descs_tokens = [[token for token in ad if token not in stopwords] for ad in ads_descs_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ads: 55449\n",
      "number of words: 7863307\n",
      "number of vocabularies: 89052\n",
      "maximum length of an ad: 1132\n",
      "minimum length of an ad: 7\n"
     ]
    }
   ],
   "source": [
    "tokens_stats(ads_descs_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove word only appear once base on term frequency\n",
    "words = list(chain.from_iterable(ads_descs_tokens))\n",
    "term_fd = FreqDist(words)\n",
    "less_frequent_words = set(term_fd.hapaxes())\n",
    "ads_descs_tokens = [[token for token in ad if token not in less_frequent_words] for ad in ads_descs_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ads: 55449\n",
      "number of words: 7814343\n",
      "number of vocabularies: 40088\n",
      "maximum length of an ad: 1121\n",
      "minimum length of an ad: 7\n"
     ]
    }
   ],
   "source": [
    "tokens_stats(ads_descs_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('experience', 43644),\n",
       " ('role', 34680),\n",
       " ('work', 33684),\n",
       " ('team', 32585),\n",
       " ('working', 30714),\n",
       " ('skills', 30412),\n",
       " ('client', 26899),\n",
       " ('job', 25552),\n",
       " ('business', 24739),\n",
       " ('uk', 24133),\n",
       " ('excellent', 22982),\n",
       " ('opportunity', 22678),\n",
       " ('company', 22263),\n",
       " ('management', 20620),\n",
       " ('required', 20555),\n",
       " ('development', 20223),\n",
       " ('apply', 20133),\n",
       " ('based', 19333),\n",
       " ('successful', 19118),\n",
       " ('join', 18682),\n",
       " ('www', 18421),\n",
       " ('salary', 18402),\n",
       " ('cv', 18383),\n",
       " ('support', 18286),\n",
       " ('knowledge', 17844),\n",
       " ('strong', 16475),\n",
       " ('environment', 16408),\n",
       " ('posted', 16398),\n",
       " ('jobseeking', 16342),\n",
       " ('candidate', 16304),\n",
       " ('originally', 16294),\n",
       " ('leading', 16194),\n",
       " ('high', 15922),\n",
       " ('service', 15623),\n",
       " ('manager', 15587),\n",
       " ('good', 15252),\n",
       " ('ability', 15154),\n",
       " ('including', 14857),\n",
       " ('position', 14564),\n",
       " ('services', 14501),\n",
       " ('benefits', 14434),\n",
       " ('training', 14218),\n",
       " ('essential', 13915),\n",
       " ('experienced', 13826),\n",
       " ('key', 13567),\n",
       " ('contact', 13551),\n",
       " ('level', 13523),\n",
       " ('recruitment', 13518),\n",
       " ('candidates', 13462),\n",
       " ('provide', 13204)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove top 50 most frequent words base on document frequency\n",
    "ads_descs_tokens_docfd = [list(set(ad)) for ad in ads_descs_tokens]\n",
    "words = list(chain.from_iterable(ads_descs_tokens_docfd))\n",
    "doc_fd = FreqDist(words)\n",
    "doc_fd.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_50 = [word[0] for word in doc_fd.most_common(50)]\n",
    "ads_descs_tokens = [[token for token in ad if token not in most_common_50] for ad in ads_descs_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ads: 55449\n",
      "number of words: 6239169\n",
      "number of vocabularies: 40038\n",
      "maximum length of an ad: 990\n",
      "minimum length of an ad: 4\n"
     ]
    }
   ],
   "source": [
    "tokens_stats(ads_descs_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('employment', 'agency'), 8055),\n",
       " (('track', 'record'), 5472),\n",
       " (('acting', 'employment'), 5095),\n",
       " (('sql', 'server'), 4804),\n",
       " (('asp', 'net'), 4687),\n",
       " (('relation', 'vacancy'), 3977),\n",
       " (('sales', 'executive'), 3619),\n",
       " (('chef', 'de'), 3586),\n",
       " (('nursing', 'home'), 3503),\n",
       " (('de', 'partie'), 3396)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the top 10 Biggrams based on term frequency\n",
    "words = list(chain.from_iterable(ads_descs_tokens))\n",
    "bigrams = ngrams(words,2)\n",
    "bigrams_fd = FreqDist(bigrams)\n",
    "bigrams_fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'employment agency': 8055,\n",
       " 'track record': 5472,\n",
       " 'acting employment': 5095,\n",
       " 'sql server': 4804,\n",
       " 'asp net': 4687,\n",
       " 'relation vacancy': 3977,\n",
       " 'sales executive': 3619,\n",
       " 'chef de': 3586,\n",
       " 'nursing home': 3503,\n",
       " 'de partie': 3396}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a dictionarty to store the bigrams\n",
    "bigrams_dict = {}\n",
    "\n",
    "for bigram in bigrams_fd.most_common(10):\n",
    "    key = bigram[0][0]+' '+bigram[0][1]\n",
    "    value = bigram[1]\n",
    "    bigrams_dict[key] = value\n",
    "\n",
    "bigrams_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs\n",
    "Save the vocabulary, bigrams and job advertisment txt as per spectification.\n",
    "- vocab.txt\n",
    "- bigram.txt\n",
    "- job_ads.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save bigrams to txt file\n",
    "bigrams_text_list = [key + ','+str(value) for key, value in bigrams_dict.items()]\n",
    "bigrams_text = '\\n'.join(bigrams_text_list)\n",
    "    \n",
    "with open('bigram.txt', 'w') as f:\n",
    "    f.write(bigrams_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all job ads\n",
    "job_ads_text_list = ['ID: ' + ads_ids[i] + '\\n'\n",
    "                    + 'Category: ' + ads_categories[i] + '\\n'\n",
    "                    + 'Webindex: '+ ads_webinds[i] + '\\n'\n",
    "                    + 'Title: ' + ads_titles[i] + '\\n'\n",
    "                    + 'Description: ' + ' '.join(ads_descs_tokens[i])\n",
    "                    for i in range(len(ads_ids))]\n",
    "\n",
    "job_ads_text = '\\n'.join(job_ads_text_list)\n",
    "    \n",
    "with open('job_ads.txt', 'w') as f:\n",
    "    f.write(job_ads_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the vocabulories\n",
    "vocabs = set(chain.from_iterable(ads_descs_tokens))\n",
    "vocabs = sorted(vocabs)\n",
    "\n",
    "vocabs_text_list = [vocabs[i] + ':' + str(i) for i in range(len(vocabs))]\n",
    "vocabs_text = '\\n'.join(vocabs_text_list)\n",
    "\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    f.write(vocabs_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Basic text processing was done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
